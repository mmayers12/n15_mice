{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy Analysis\n",
    "\n",
    "This runs the backend of the taxonomy analysis and generates Dataframes that have tax data at Phylum and Family levels.\n",
    "\n",
    "For each peptide in a sample, it searches the database to find the LCA for that peptide.  If it can be assigned at the Phylum or Family level, then it is assigned for the level and given a weight which is equal to the number of spectral counts for that peptide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import shelve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from scripts import utils\n",
    "from scripts.analysis import build_loci\n",
    "from scripts.analysis import taxonomy\n",
    "from scripts.analysis import DBInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 2,\n",
       " 'exact_match': True,\n",
       " 'matched_organism': 'Paenibacillus phage phiIBB_Pl23',\n",
       " 'organism': 'Paenibacillus phage phiIBB_Pl23',\n",
       " 'taxid': 1337877}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start and test DB connections\n",
    "dbinfo = DBInfo.DBInfo('compil_mgm')\n",
    "t = taxonomy.Taxonomy(host=\"wl-cmadmin\", port=27017)\n",
    "protDB = dbinfo.protDB\n",
    "seqDB = dbinfo.seqDB\n",
    "taxDB = MongoClient('wl-cmadmin', 27017)[\"TaxDB_20151009\"][\"TaxDB_20151009\"]\n",
    "taxDB.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "BASE = '../data'\n",
    "OUT = BASE + '/tax_data'\n",
    "\n",
    "samples = shelve.open(os.path.join(BASE, 'samples.shelve'))\n",
    "protein_clusters = shelve.open(os.path.join(BASE, 'protein_clusters.shelve'))\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(BASE,\"filt_metadata.csv\"), index_col=0)\n",
    "samp_names = list(metadata.index)\n",
    "\n",
    "met1 = pd.read_csv(os.path.join(BASE, 'filt_metadata.csv'), index_col=0)\n",
    "\n",
    "unenr_grouped_loci = utils.load(os.path.join(BASE,\"unenriched_grouped_loci.pkl.gz\"))\n",
    "enr_grouped_loci = utils.load(os.path.join(BASE,\"enriched_grouped_loci.pkl.gz\"))\n",
    "grouped_loci = utils.load(os.path.join(BASE,\"grouped_loci.pkl.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out unused samples\n",
    "samps = {k:v for k, v in samples.items() if k in samp_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_pairs = metadata.reset_index().set_index(['enriched', 'technical']).sort_index().groupby(level=[0,1])\n",
    "\n",
    "pairs = []\n",
    "for x, y in sample_pairs:\n",
    "    pairs.append(list(y['index'].values))\n",
    "\n",
    "n14_samps = [x[0] for x in pairs]\n",
    "n15_samps = [x[1] for x in pairs]\n",
    "\n",
    "n14_un_samps = [x for x in n14_samps if x.startswith('UL_')]\n",
    "n15_un_samps = [x for x in n15_samps if x.startswith('UL_')]\n",
    "\n",
    "n14_enr_samps = [x for x in n14_samps if x.startswith('CL_')]\n",
    "n15_enr_samps = [x for x in n15_samps if x.startswith('CL_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctl_samps = []\n",
    "for samp_name, samp in samples.items():\n",
    "    if samp['probe'] == 'DMSO':\n",
    "        ctl_samps.append(samp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "peptides_to_filter = []\n",
    "for sample in ctl_samps:\n",
    "    for locus in protein_clusters[sample]:\n",
    "        if locus.quantification['counts'] >= 10:\n",
    "            peptides_to_filter += list(locus.peptide_quant.keys())\n",
    "peptides_to_filter = set(peptides_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = dict()\n",
    "dfs[\"phylum\"] = dict()\n",
    "dfs[\"family\"] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [00:00<00:00, 2149.56it/s]\n",
      "100%|██████████| 1417/1417 [00:00<00:00, 4095.10it/s]\n",
      "100%|██████████| 1360/1360 [00:00<00:00, 3962.44it/s]\n",
      "100%|██████████| 1165/1165 [00:00<00:00, 2413.40it/s]\n",
      "100%|██████████| 2682/2682 [00:00<00:00, 3731.16it/s]\n",
      "100%|██████████| 1591/1591 [00:00<00:00, 3635.17it/s]\n",
      "100%|██████████| 1609/1609 [00:00<00:00, 3778.98it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2244.72it/s]\n",
      "100%|██████████| 1677/1677 [00:00<00:00, 2409.37it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 4009.22it/s]\n",
      "100%|██████████| 2236/2236 [00:00<00:00, 4173.23it/s]\n",
      "100%|██████████| 1044/1044 [00:00<00:00, 2561.82it/s]\n",
      "100%|██████████| 2079/2079 [00:00<00:00, 3657.24it/s]\n",
      "100%|██████████| 1570/1570 [00:00<00:00, 3012.20it/s]\n",
      "100%|██████████| 2194/2194 [00:00<00:00, 3793.10it/s]\n",
      "100%|██████████| 1628/1628 [00:00<00:00, 2498.78it/s]\n",
      "100%|██████████| 1697/1697 [00:00<00:00, 3682.81it/s]\n",
      "100%|██████████| 1831/1831 [00:00<00:00, 4090.80it/s]\n",
      "100%|██████████| 948/948 [00:00<00:00, 2236.66it/s]\n",
      "100%|██████████| 1311/1311 [00:00<00:00, 2477.96it/s]\n",
      "100%|██████████| 1687/1687 [00:00<00:00, 2450.60it/s]\n",
      "100%|██████████| 2441/2441 [00:00<00:00, 4138.38it/s]\n",
      "100%|██████████| 1709/1709 [00:00<00:00, 2221.46it/s]\n",
      "100%|██████████| 1247/1247 [00:00<00:00, 2406.80it/s]\n",
      "100%|██████████| 934/934 [00:00<00:00, 2236.45it/s]\n",
      "100%|██████████| 3168/3168 [00:00<00:00, 4163.60it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 4171.32it/s]\n",
      "100%|██████████| 1230/1230 [00:00<00:00, 2429.75it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_pep_quant = defaultdict(dict)\n",
    "sample_sum = dict()\n",
    "for sample_name, sample in samps.items():\n",
    "    # Probably a better way to do this than looping, but I'm pressed for time, Hopefully won't have to re-run lots of times, otherwise a better algortihm should be used\n",
    "    for pep in tqdm(sample.pep_quant.columns):\n",
    "        # Calculate the back_calculated counts for a peptide.\n",
    "        if sample['n15']:\n",
    "            if not np.isnan(sample.pep_quant.loc['ratio', pep]) and sample.pep_quant.loc['l_spec', pep] != 0:\n",
    "                back_calc = sample.pep_quant.loc['l_spec', pep] / sample.pep_quant.loc['ratio', pep]\n",
    "            else:\n",
    "                back_calc = sample.pep_quant.loc['h_spec', pep]\n",
    "        else:\n",
    "            if sample.pep_quant.loc['l_spec', pep] == 0 and not np.isnan(sample.pep_quant.loc['ratio', pep]):\n",
    "                back_calc = sample.pep_quant.loc['h_spec', pep] * sample.pep_quant.loc['ratio', pep]\n",
    "            else:\n",
    "                back_calc = sample.pep_quant.loc['l_spec', pep]\n",
    "        \n",
    "        # Store these back counts as quant for the pep\n",
    "        sample_pep_quant[sample_name].update({pep: back_calc})\n",
    "    \n",
    "    sample_sum[sample_name] = sum(sample_pep_quant[sample_name].values())\n",
    "nf = {k:v/np.median(list(sample_sum.values())) for k,v in sample_sum.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 967/967 [00:03<00:00, 316.67it/s]\n",
      "100%|██████████| 788/788 [00:00<00:00, 1703.79it/s]\n",
      "100%|██████████| 788/788 [00:00<00:00, 1317827.57it/s]\n",
      "100%|██████████| 788/788 [00:06<00:00, 113.35it/s]\n",
      "100%|██████████| 788/788 [00:03<00:00, 243.92it/s]\n",
      "100%|██████████| 1174/1174 [00:02<00:00, 403.45it/s]\n",
      "100%|██████████| 1119/1119 [00:00<00:00, 1792.48it/s]\n",
      "100%|██████████| 1119/1119 [00:00<00:00, 1319861.13it/s]\n",
      "100%|██████████| 1119/1119 [00:10<00:00, 102.68it/s]\n",
      "100%|██████████| 1119/1119 [00:04<00:00, 249.17it/s]\n",
      "100%|██████████| 1360/1360 [00:03<00:00, 431.77it/s]\n",
      "100%|██████████| 1256/1256 [00:00<00:00, 2803.16it/s]\n",
      "100%|██████████| 1256/1256 [00:00<00:00, 1292137.80it/s]\n",
      "100%|██████████| 1256/1256 [00:16<00:00, 74.50it/s]\n",
      "100%|██████████| 1256/1256 [00:05<00:00, 239.77it/s]\n",
      "100%|██████████| 1031/1031 [00:02<00:00, 424.54it/s]\n",
      "100%|██████████| 831/831 [00:00<00:00, 2314.15it/s]\n",
      "100%|██████████| 831/831 [00:00<00:00, 1415705.37it/s]\n",
      "100%|██████████| 831/831 [00:07<00:00, 116.00it/s]\n",
      "100%|██████████| 831/831 [00:03<00:00, 256.35it/s]\n",
      "100%|██████████| 2682/2682 [00:07<00:00, 362.17it/s]\n",
      "100%|██████████| 2589/2589 [00:01<00:00, 2097.85it/s]\n",
      "100%|██████████| 2589/2589 [00:00<00:00, 1115006.99it/s]\n",
      "100%|██████████| 2589/2589 [00:27<00:00, 93.32it/s]\n",
      "100%|██████████| 2589/2589 [00:09<00:00, 268.59it/s]\n",
      "100%|██████████| 1591/1591 [00:03<00:00, 415.92it/s]\n",
      "100%|██████████| 1487/1487 [00:00<00:00, 2045.92it/s]\n",
      "100%|██████████| 1487/1487 [00:00<00:00, 1278583.45it/s]\n",
      "100%|██████████| 1487/1487 [00:21<00:00, 69.77it/s]\n",
      "100%|██████████| 1487/1487 [00:07<00:00, 211.85it/s]\n",
      "100%|██████████| 1609/1609 [00:04<00:00, 351.40it/s]\n",
      "100%|██████████| 1481/1481 [00:00<00:00, 2063.47it/s]\n",
      "100%|██████████| 1481/1481 [00:00<00:00, 1324752.45it/s]\n",
      "100%|██████████| 1481/1481 [00:18<00:00, 80.13it/s]\n",
      "100%|██████████| 1481/1481 [00:05<00:00, 249.08it/s]\n",
      "100%|██████████| 1084/1084 [00:02<00:00, 405.47it/s]\n",
      "100%|██████████| 981/981 [00:00<00:00, 2137.10it/s]\n",
      "100%|██████████| 981/981 [00:00<00:00, 1364261.35it/s]\n",
      "100%|██████████| 981/981 [00:09<00:00, 113.89it/s]\n",
      "100%|██████████| 981/981 [00:04<00:00, 241.64it/s]\n",
      "100%|██████████| 1677/1677 [00:04<00:00, 364.47it/s]\n",
      "100%|██████████| 1493/1493 [00:00<00:00, 1792.26it/s]\n",
      "100%|██████████| 1493/1493 [00:00<00:00, 1357194.60it/s]\n",
      "100%|██████████| 1493/1493 [00:14<00:00, 103.27it/s]\n",
      "100%|██████████| 1493/1493 [00:05<00:00, 269.57it/s]\n",
      "100%|██████████| 2700/2700 [00:08<00:00, 321.90it/s]\n",
      "100%|██████████| 2598/2598 [00:01<00:00, 1397.24it/s]\n",
      "100%|██████████| 2598/2598 [00:00<00:00, 1308139.47it/s]\n",
      "100%|██████████| 2598/2598 [00:22<00:00, 114.92it/s]\n",
      "100%|██████████| 2598/2598 [00:08<00:00, 306.23it/s]\n",
      "100%|██████████| 1891/1891 [00:04<00:00, 389.21it/s]\n",
      "100%|██████████| 1854/1854 [00:00<00:00, 2269.16it/s]\n",
      "100%|██████████| 1854/1854 [00:00<00:00, 1361148.19it/s]\n",
      "100%|██████████| 1854/1854 [00:17<00:00, 106.66it/s]\n",
      "100%|██████████| 1854/1854 [00:07<00:00, 256.89it/s]\n",
      "100%|██████████| 1044/1044 [00:02<00:00, 416.79it/s]\n",
      "100%|██████████| 913/913 [00:00<00:00, 2702.40it/s]\n",
      "100%|██████████| 913/913 [00:00<00:00, 1198185.09it/s]\n",
      "100%|██████████| 913/913 [00:12<00:00, 72.30it/s]\n",
      "100%|██████████| 913/913 [00:03<00:00, 248.28it/s]\n",
      "100%|██████████| 2079/2079 [00:05<00:00, 393.18it/s]\n",
      "100%|██████████| 1984/1984 [00:01<00:00, 1759.46it/s]\n",
      "100%|██████████| 1984/1984 [00:00<00:00, 1041489.25it/s]\n",
      "100%|██████████| 1984/1984 [00:26<00:00, 75.44it/s]\n",
      "100%|██████████| 1984/1984 [00:08<00:00, 220.54it/s]\n",
      "100%|██████████| 1570/1570 [00:04<00:00, 371.79it/s]\n",
      "100%|██████████| 1462/1462 [00:00<00:00, 2401.36it/s]\n",
      "100%|██████████| 1462/1462 [00:00<00:00, 1357253.75it/s]\n",
      "100%|██████████| 1462/1462 [00:14<00:00, 104.19it/s]\n",
      "100%|██████████| 1462/1462 [00:05<00:00, 275.49it/s]\n",
      "100%|██████████| 2194/2194 [00:05<00:00, 398.40it/s]\n",
      "100%|██████████| 2074/2074 [00:00<00:00, 2277.45it/s]\n",
      "100%|██████████| 2074/2074 [00:00<00:00, 1250752.91it/s]\n",
      "100%|██████████| 2074/2074 [00:25<00:00, 82.28it/s]\n",
      "100%|██████████| 2074/2074 [00:08<00:00, 249.71it/s]\n",
      "100%|██████████| 1628/1628 [00:03<00:00, 415.22it/s]\n",
      "100%|██████████| 1532/1532 [00:00<00:00, 2347.80it/s]\n",
      "100%|██████████| 1532/1532 [00:00<00:00, 1259689.03it/s]\n",
      "100%|██████████| 1532/1532 [00:17<00:00, 86.59it/s]\n",
      "100%|██████████| 1532/1532 [00:06<00:00, 234.79it/s]\n",
      "100%|██████████| 1697/1697 [00:04<00:00, 419.12it/s]\n",
      "100%|██████████| 1618/1618 [00:00<00:00, 2352.10it/s]\n",
      "100%|██████████| 1618/1618 [00:00<00:00, 1260004.43it/s]\n",
      "100%|██████████| 1618/1618 [00:22<00:00, 72.50it/s]\n",
      "100%|██████████| 1618/1618 [00:07<00:00, 216.42it/s]\n",
      "100%|██████████| 1551/1551 [00:03<00:00, 436.78it/s]\n",
      "100%|██████████| 1479/1479 [00:00<00:00, 2471.47it/s]\n",
      "100%|██████████| 1479/1479 [00:00<00:00, 1178452.81it/s]\n",
      "100%|██████████| 1479/1479 [00:12<00:00, 116.85it/s]\n",
      "100%|██████████| 1479/1479 [00:04<00:00, 308.43it/s]\n",
      "100%|██████████| 835/835 [00:02<00:00, 403.79it/s]\n",
      "100%|██████████| 754/754 [00:00<00:00, 2319.50it/s]\n",
      "100%|██████████| 754/754 [00:00<00:00, 1295577.72it/s]\n",
      "100%|██████████| 754/754 [00:07<00:00, 104.01it/s]\n",
      "100%|██████████| 754/754 [00:03<00:00, 247.38it/s]\n",
      "100%|██████████| 1311/1311 [00:03<00:00, 411.27it/s]\n",
      "100%|██████████| 1212/1212 [00:00<00:00, 2367.81it/s]\n",
      "100%|██████████| 1212/1212 [00:00<00:00, 1285334.12it/s]\n",
      "100%|██████████| 1212/1212 [00:14<00:00, 84.83it/s]\n",
      "100%|██████████| 1212/1212 [00:05<00:00, 235.78it/s]\n",
      "100%|██████████| 1687/1687 [00:04<00:00, 378.56it/s]\n",
      "100%|██████████| 1548/1548 [00:00<00:00, 2151.40it/s]\n",
      "100%|██████████| 1548/1548 [00:00<00:00, 1285699.52it/s]\n",
      "100%|██████████| 1548/1548 [00:16<00:00, 94.64it/s]\n",
      "100%|██████████| 1548/1548 [00:05<00:00, 261.15it/s]\n",
      "100%|██████████| 2197/2197 [00:06<00:00, 359.04it/s]\n",
      "100%|██████████| 2126/2126 [00:01<00:00, 1760.50it/s]\n",
      "100%|██████████| 2126/2126 [00:00<00:00, 1379500.36it/s]\n",
      "100%|██████████| 2126/2126 [00:17<00:00, 121.78it/s]\n",
      "100%|██████████| 2126/2126 [00:08<00:00, 247.69it/s]\n",
      "100%|██████████| 1562/1562 [00:03<00:00, 412.79it/s]\n",
      "100%|██████████| 1406/1406 [00:00<00:00, 2382.27it/s]\n",
      "100%|██████████| 1406/1406 [00:00<00:00, 1370483.71it/s]\n",
      "100%|██████████| 1406/1406 [00:12<00:00, 108.71it/s]\n",
      "100%|██████████| 1406/1406 [00:05<00:00, 251.62it/s]\n",
      "100%|██████████| 1247/1247 [00:03<00:00, 398.44it/s]\n",
      "100%|██████████| 1135/1135 [00:00<00:00, 2112.03it/s]\n",
      "100%|██████████| 1135/1135 [00:00<00:00, 1336853.42it/s]\n",
      "100%|██████████| 1135/1135 [00:14<00:00, 76.88it/s]\n",
      "100%|██████████| 1135/1135 [00:05<00:00, 213.54it/s]\n",
      "100%|██████████| 809/809 [00:01<00:00, 448.60it/s]\n",
      "100%|██████████| 652/652 [00:00<00:00, 3282.98it/s]\n",
      "100%|██████████| 652/652 [00:00<00:00, 1378370.06it/s]\n",
      "100%|██████████| 652/652 [00:06<00:00, 96.88it/s] \n",
      "100%|██████████| 652/652 [00:02<00:00, 241.00it/s]\n",
      "100%|██████████| 2784/2784 [00:07<00:00, 387.76it/s]\n",
      "100%|██████████| 2700/2700 [00:01<00:00, 1954.57it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 1284407.49it/s]\n",
      "100%|██████████| 2700/2700 [00:22<00:00, 122.17it/s]\n",
      "100%|██████████| 2700/2700 [00:09<00:00, 297.02it/s]\n",
      "100%|██████████| 2622/2622 [00:07<00:00, 364.19it/s]\n",
      "100%|██████████| 2558/2558 [00:01<00:00, 2169.00it/s]\n",
      "100%|██████████| 2558/2558 [00:00<00:00, 1410229.97it/s]\n",
      "100%|██████████| 2558/2558 [00:19<00:00, 126.68it/s]\n",
      "100%|██████████| 2558/2558 [00:07<00:00, 336.02it/s]\n",
      "100%|██████████| 1230/1230 [00:03<00:00, 347.90it/s]\n",
      "100%|██████████| 1076/1076 [00:00<00:00, 1906.26it/s]\n",
      "100%|██████████| 1076/1076 [00:00<00:00, 1046873.37it/s]\n",
      "100%|██████████| 1076/1076 [00:11<00:00, 92.54it/s]\n",
      "100%|██████████| 1076/1076 [00:04<00:00, 251.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for sample_name, sample in samps.items():\n",
    "    \n",
    "    pep_tax = dict()\n",
    "    peptides = sample.peptides\n",
    "    \n",
    "    #Get rid of control peptides if the sample is enriched.\n",
    "    if sample['enriched']:\n",
    "        peptides = set(pep for pep in peptides if not pep in peptides_to_filter)\n",
    "    \n",
    "    #Do any peptides, by themselves (but restricted to the proteins ID'd in this sample), point to one particular species?\n",
    "    for peptide in tqdm(peptides):\n",
    "        p = set(x['i'] for x in seqDB.find_one(peptide)['p']) & sample.prot_ids\n",
    "        x = list(taxDB.aggregate([{'$match':{'_id':{'$in':list(p)}}},{'$group':{'_id':None,'taxid':{'$addToSet':\"$taxid\"}}}]))\n",
    "        if x:\n",
    "            taxIDs=[y for y in x[0]['taxid'] if y]\n",
    "            pep_tax[peptide] = taxIDs\n",
    "    \n",
    "    all_taxids = set(chain(*pep_tax.values()))\n",
    "    pep_lca = dict()\n",
    "    for pep, taxIDs in tqdm(pep_tax.items()):\n",
    "        pep_lca[pep] = t.LCA(taxIDs)\n",
    "    \n",
    "    pep_lcatax = dict()\n",
    "    all_taxids = set(pep_lca.values())\n",
    "    taxdict = {x['taxid']:x for x in t.taxonomy_coll.find({'taxid': {'$in': list(all_taxids)}})}\n",
    "    for pep, lca in tqdm(pep_lca.items()):\n",
    "        if lca:\n",
    "            tax = taxdict[lca]\n",
    "            if tax['rank#'] and tax['rank#']>=16:\n",
    "                pep_lcatax[pep] = tax\n",
    "    #\n",
    "    species_pep = defaultdict(set)\n",
    "    pep_species = {pep:tax['scientific_name'] for pep,tax in pep_lcatax.items()}\n",
    "    for pep,species in pep_species.items():\n",
    "        species_pep[species].add(pep)\n",
    "            \n",
    "    species_pep_list = sorted(list(species_pep.items()), key=lambda x:len(x[1]), reverse=True)\n",
    "    \n",
    "    with open(os.path.join(OUT,\"organims_specific_peptides_{}.csv\".format(sample_name)), 'w') as f:\n",
    "        f.writelines('\\n'.join([x[0] + \";\" + \",\".join(x[1]) for x in species_pep_list]))\n",
    "    \n",
    "    # Take every peptide and bring it up to family\n",
    "    # Quantify family counts\n",
    "    for rank in [\"phylum\", \"family\"]:\n",
    "        organism_pep = defaultdict(set)\n",
    "        for pep, lca in tqdm(pep_lca.items()):\n",
    "            family = t.get_rank(lca, rank, \"taxid\")\n",
    "            if family:\n",
    "                organism_pep[family].add(pep)\n",
    "        \n",
    "        organism_quant = {k:sum([sample_pep_quant[sample_name][pep] for pep in peps]) for k,peps in organism_pep.items()}\n",
    "        \n",
    "        df = pd.DataFrame(organism_quant, index = ['count']).T\n",
    "        df['organism_name'] = [t.taxid_to_taxonomy(int(x))['scientific_name'] for x in df.index]\n",
    "        df=df.sort_values(\"count\",ascending=False)\n",
    "        \n",
    "        df.to_csv(os.path.join(OUT,\"{}_count_{}.csv\".format(rank, sample_name)))\n",
    "        dfs[rank][sample_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rank in [\"phylum\", \"family\"]:\n",
    "    for sample_name, df in dfs[rank].items():\n",
    "        df['taxid'] = df.index\n",
    "        df['sample'] = sample_name\n",
    "    all_df = pd.concat(list(dfs[rank].values()))\n",
    "    all_df.to_csv(os.path.join(OUT,\"{}_count.csv\".format(rank)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
